---
title: "Maximizing mail return rates in a charitable organization – Applied Data Mining & Statistical Learning"
author: "Xiang Wang"
date: "December 14, 2015"
output: html_document
---

### Introduction
A charitable organization wishes to develop a data-mining model to improve the cost-effectiveness of their direct marketing campaigns to previous donors. According to their recent mailing records, the typical overall response rate is 10%. Out of those who respond (donate), the average donation is $14.50. Each mailing, which includes a gift of personalized address labels and assortments of cards and envelopes, costs $2 to produce and send. Since expected profit from each mailing is 14.5  0.1 – 2 = –$0.55, it is not cost effective to mail everyone. 

We would like to develop a classification model using data from the most recent campaign that can effectively capture likely donors so that the expected net profit is maximized. The entire dataset consists of 3984 training observations, 2018 validation observations, and 2007 test observations. Weighted sampling has been used, over- representing the responders so that the training and validation samples have approximately equal numbers of donors and non-donors. The response rate in the test sample has the more typical 10% response rate. We would also like to build a model to predict donation amounts for donors – the data for this will consist of the records for donors only. The data are available in the file “charity.csv”:

- ID number
- REG1, REG2, REG3, REG4: Region (There are five geographic regions; only four are needed for analysis since if a potential donor falls into none of the four he or she must be in the other region. Inclusion of all five indicator variables would be redundant and cause some modeling techniques to fail. A “1” indicates the potential donor belongs to this region.)
- HOME: (1 = homeowner, 0 = not a homeowner)
- CHLD: Number of children
- HINC: Household income (7 categories)
- GENF: Gender (0 = Male, 1 = Female)
- WRAT: Wealth Rating (Wealth rating uses median family income and
population statistics from each area to index relative wealth within each state. The segments are denoted 0-9, with 9 being the highest wealth group and 0 being the lowest.)
- AVHV: Average Home Value in potential donor's neighborhood in $ thousands
- INCM: Median Family Income in potential donor's neighborhood in $ thousands
- INCA: Average Family Income in potential donor's neighborhood in $ thousands
- PLOW: Percent categorized as “low income” in potential donor's neighborhood
- NPRO: Lifetime number of promotions received to date
- TGIF: Dollar amount of lifetime gifts to date
- LGIF: Dollar amount of largest gift to date
- RGIF: Dollar amount of most recent gift
- TDON: Number of months since last donation
- TLAG: Number of months between first and second gift
- AGIF: Average dollar amount of gifts to date
- DONR: Classification Response Variable (1 = Donor, 0 = Non-donor)
- DAMT: Prediction Response Variable (Donation Amount in $)

The DONR and DAMT variables are set to “NA” for the test set. 

### Project Objectives
1. Develop a **classification model** for the DONR variable using any of the variables as predictors (except ID and DAMT). Fit all candidate models using the training data and evaluate the fitted models using the validation data. Use “maximum profit” as the evaluation criteria and use your final selected classification model to classify DONR responses in the test dataset.

2. Develop a **prediction model** for the DAMT variable using any of the variables as predictors (except ID and DONR). Use only the data records for which DONR=1. Fit all candidate models using the training data and evaluate the fitted models using the validation data. Use “mean prediction error” as the evaluation criteria and use your final selected prediction model to predict DAMT responses in the test dataset.

### Exploratory Data Analysis
```{r}
charity <- read.csv("charity.csv"); str(charity)
charity1 <- charity[, c(7:8, 10:21, 24)] 
## Matrix scatter plot using ggpairs() 
library(GGally); # ggpairs(charity1, colour='part') 
## This plot can give us a lot information. For the reason of multi-collinearity, we can discard variables "incm, inca, plow" as these three variables are all strongly correlated with avhv and they are correlated to each other. In addition, we can discard tgif as it strongly correlates to npro; we can also discard lgif and agif as they both strongly correlate to rgif. The predictor variables to be used are: reg1, reg2, reg3, reg4, home, chld, hinc, genf, wrat, avhv, npro, rgif, tdon, tlag.

## We can also check the distribution/normality of each variable by train, validation or test data through the lower part of the plot. Of the above chosen variables, avhv, rgif and tlag are skewed to the right. After log-transformation, they are normally distributed. chld and wrat are not normally distributed, but I am NOT thinking to transform them now since they are more categorical/ordinal. Transformation does not help too much.

## Below are the histograms after transformation. All look normally distributed.
hist(log(charity$avhv)) 
hist(log(charity$rgif))
hist(log(charity$tlag)) 

# transformations
charity.t <- charity
charity.t$avhv <- log(charity.t$avhv) 
charity.t$rgif <- log(charity.t$rgif)
charity.t$tlag <- log(charity.t$tlag)

# set up data for analysis
data.train <- charity.t[charity$part=="train",]
x.train <- data.train[,2:21]
c.train <- data.train[,22] # donr
n.train.c <- length(c.train) # 3984
y.train <- data.train[c.train==1,23] # damt for observations with donr=1
n.train.y <- length(y.train) # 1995

data.valid <- charity.t[charity$part=="valid",]
x.valid <- data.valid[,2:21]
c.valid <- data.valid[,22] # donr
n.valid.c <- length(c.valid) # 2018
y.valid <- data.valid[c.valid==1,23] # damt for observations with donr=1
n.valid.y <- length(y.valid) # 999

data.test <- charity.t[charity$part=="test",]
n.test <- dim(data.test)[1] # 2007
x.test <- data.test[,2:21]

x.train.mean <- apply(x.train, 2, mean)
x.train.sd <- apply(x.train, 2, sd)
x.train.std <- t((t(x.train)-x.train.mean)/x.train.sd) # standardize to have zero mean and unit sd
apply(x.train.std, 2, mean) # check zero mean
apply(x.train.std, 2, sd) # check unit sd
data.train.std.c <- data.frame(x.train.std, donr=c.train) # to classify donr
data.train.std.y <- data.frame(x.train.std[c.train==1,], damt=y.train) # to predict damt when donr=1

x.valid.std <- t((t(x.valid)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.valid.std.c <- data.frame(x.valid.std, donr=c.valid) # to classify donr
data.valid.std.y <- data.frame(x.valid.std[c.valid==1,], damt=y.valid) # to predict damt when donr=1

x.test.std <- t((t(x.test)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.test.std <- data.frame(x.test.std)
```

### Classification modeling

#### Model 1: linear discriminant analysis (LDA)
```{r}
library(MASS)
model.lda <- lda(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + wrat + avhv + npro + rgif + tdon + tlag, data.train.std.c) # include additional terms on the fly using I()
# Note: strictly speaking, LDA should not be used with qualitative predictors, but in practice it often is if the goal is simply to find a good predictive model
post.valid.lda <- predict(model.lda, data.valid.std.c)$posterior[,2] # n.valid.c post probs
# calculate ordered profit function using average donation = $14.50 and mailing cost = $2
profit.lda <- cumsum(14.5*c.valid[order(post.valid.lda, decreasing=T)]-2)
plot(profit.lda) # see how profits change as more mailings are made
n.mail.valid <- which.max(profit.lda) # number of mailings that maximizes profits
c(n.mail.valid, max(profit.lda)) # report number of mailings and maximum profit; 1345, 11621.5

cutoff.lda <- sort(post.valid.lda, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.lda <- ifelse(post.valid.lda > cutoff.lda, 1, 0) # mail to everyone above the cutoff
table(chat.valid.lda, c.valid) # classification table
# check n.mail.valid = 358+987 = 1345
# check profit = 14.5*987-2*1345 = 11621.5
```

#### Model 2: logistic regression (LR)
```{r}
model.log <- glm(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + wrat + avhv + npro + rgif + tdon + tlag, data.train.std.c, family=binomial("logit"))

post.valid.log <- predict(model.log, data.valid.std.c, type="response") # n.valid post probs
# calculate ordered profit function using average donation = $14.50 and mailing cost = $2
profit.log <- cumsum(14.5*c.valid[order(post.valid.log, decreasing=T)]-2)
plot(profit.log) # see how profits change as more mailings are made
n.mail.valid <- which.max(profit.log) # number of mailings that maximizes profits
c(n.mail.valid, max(profit.log)) # report number of mailings and maximum profit; 1273, 11635

cutoff.log <- sort(post.valid.log, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.log <- ifelse(post.valid.log > cutoff.log, 1, 0) # mail to everyone above the cutoff
table(chat.valid.log, c.valid) # classification table
# check n.mail.valid = 295+978 = 1273
# check profit = 14.5*978-2*1273 = 11635
```

#### Model 3:  Quadratic discriminant analysis (QDA)
```{r}
model.qda <- qda(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + wrat + avhv + npro + rgif + tdon + tlag, data.train.std.c) 
post.valid.qda <- predict(model.qda, data.valid.std.c)$posterior[,2] # n.valid.c post probs
# calculate ordered profit function using average donation = $14.50 and mailing cost = $2
profit.qda <- cumsum(14.5*c.valid[order(post.valid.qda, decreasing=T)]-2)
plot(profit.qda) # see how profits change as more mailings are made
n.mail.valid <- which.max(profit.qda) # number of mailings that maximizes profits
c(n.mail.valid, max(profit.qda)) # report number of mailings and maximum profit  1391.0 11210.5

cutoff.qda <- sort(post.valid.qda, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.qda <- ifelse(post.valid.qda>cutoff.qda, 1, 0) # mail to everyone above the cutoff
table(chat.valid.qda, c.valid) # classification table
# check n.mail.valid = 426+965 = 1391
# check profit = 14.5*965-2*1391 = 11210.5
```

#### Model 4: K-Nearest Neighbors (KNN)
```{r}
library(class)
mer <- rep(NA, 30) # misclassification error rates based on leave-one-out cross-validation
set.seed(2014) # seed must be set because R randomly breaks ties
for (i in 1:30) mer[i] <- sum((c.train-(c(knn.cv(train=x.train, cl=c.train, k=i))-1))^2)/n.train.c
which.min(mer) # minimum occurs at k=29

set.seed(2014)
post.valid.knn <- knn(data.train.std.c[,-21], data.valid.std.c[,-21], data.train.std.c[,21], k=29, prob=T)
# calculate ordered profit function using average donation = $14.50 and mailing cost = $2
profit.knn <- cumsum(14.5*c.valid[order(post.valid.knn, decreasing=T)]-2)
plot(profit.knn) # see how profits change as more mailings are made
n.mail.valid <- which.max(profit.knn) # number of mailings that maximizes profits
c(n.mail.valid, max(profit.knn)) # report number of mailings and maximum profit; 1305, 11208.5
```

#### Model 5: Generalized additive model (GAM) with each predictor modeled using a smoothing spline with 5 degrees of freedom
```{r}
library(gam)
model.gam <- gam(donr ~ reg1 + reg2 + reg3 + reg4 + home + s(chld,df=5) + s(hinc,df=5) + genf + s(wrat,df=5) + s(avhv,df=5) + s(incm,df=5) + s(inca,df=5) + s(plow,df=5) + s(npro,df=5) + s(tgif,df=5) + s(lgif,df=5) + s(rgif,df=5) + s(tdon,df=5) + s(tlag,df=5) + s(agif,df=5), data.train.std.c, family=binomial)
summary(model.gam)

#remove spline for linear predictors
model.gam2 <- gam(donr ~ reg1 + reg2 + reg3 + reg4 + home + s(chld,df=5) + s(hinc,df=5) + genf + s(wrat,df=5) + avhv + s(incm,df=5) + s(inca,df=5) + s(plow,df=5) + npro + s(tgif,df=5) + lgif + rgif + s(tdon,df=5) + s(tlag,df=5) + agif, data.train.std.c, family=binomial)
summary(model.gam2)

#remove non-significant predictors
model.gam3 <- gam(donr ~ reg1 + reg2 + reg3 + reg4 + home + s(chld,df=5) + s(hinc,df=5) +  s(wrat,df=5) + avhv + s(incm,df=5) + s(inca,df=5) + s(plow,df=5) + npro + s(tgif,df=5) + s(tdon,df=5) + s(tlag,df=5), data.train.std.c, family=binomial)
summary(model.gam3)
post.valid.gam <- predict(model.gam3, data.valid.std.c, type="response") # n.valid.c post probs

# calculate ordered profit function using average donation = $14.50 and mailing cost = $2
profit.gam <- cumsum(14.5*c.valid[order(post.valid.gam, decreasing=T)]-2)
plot(profit.gam) # see how profits change as more mailings are made
n.mail.valid <- which.max(profit.gam) # number of mailings that maximizes profits
c(n.mail.valid, max(profit.gam)) # report number of mailings and maximum profit 1237 11939

cutoff.gam <- sort(post.valid.gam, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.gam <- ifelse(post.valid.gam>cutoff.gam, 1, 0) # mail to everyone above the cutoff
table(chat.valid.gam, c.valid) # classification table
# check n.mail.valid = 243+994 = 1237
# check profit = 14.5*994-2*1237 = 11939
```

#### Model 6: Regression Tree
```{r}
library(tree)
tree.charity =tree(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + wrat + avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif ,data.train.std.c)
summary(tree.charity)
plot(tree.charity); text(tree.charity ,pretty=0)
tree.pred <- predict (tree.charity ,data.valid.std.c)

profit.tree <- cumsum(14.5*c.valid[order(tree.pred, decreasing=T)]-2)
plot(profit.tree) # see how profits change as more mailings are made
n.mail.valid <- which.max(profit.tree) # number of mailings that maximizes profits
c(n.mail.valid, max(profit.tree)) # report number of mailings and maximum profit  1391 11312

cutoff.tree <- sort(tree.pred, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.tree <- ifelse(tree.pred>cutoff.tree, 1, 0) # mail to everyone above the cutoff
table(chat.valid.tree, c.valid) # classification table
# check: 374+962 = 1336
# check: 14.5*962 - 2*1336 = 11277
```

#### Model 7: Random forest (RF)
```{r}
library(randomForest); set.seed(1)
model.rf <- randomForest(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + wrat + avhv + npro + rgif + tdon + tlag, data.train.std.c, importance=TRUE)
importance(model.rf) # chld, home, reg2, wrat are among the most influence factors.
post.valid.rf <- predict(model.rf, data.valid.std.c) # n.valid post probs

# calculate ordered profit function using average donation = $14.50 and mailing cost = $2
profit.rf <- cumsum(14.5*c.valid[order(post.valid.rf, decreasing=T)]-2)
plot(profit.rf) # see how profits change as more mailings are made
n.mail.valid <- which.max(profit.rf) # number of mailings that maximizes profits
c(n.mail.valid, max(profit.rf)) # report number of mailings and maximum profit; 1256, 11799.5

cutoff.rf <- sort(post.valid.rf, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.rf <- ifelse(post.valid.rf > cutoff.rf, 1, 0) # mail to everyone above the cutoff
table(chat.valid.rf, c.valid) # classification table
# check n.mail.valid = 269+987 = 1256
# check profit = 14.5*987-2*1256 = 11799.5
```

#### Model 8: Support Vector Machine (SVM)
```{r}
library(e1071)
svm.charity <- svm(donr ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + wrat + avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif, data=data.train.std.c, kernel="linear",cost=10)
summary(svm.charity)
svm.pred <- predict (svm.charity, data.valid.std.c)

profit.svm <- cumsum(14.5*c.valid[order(svm.pred, decreasing=T)]-2)
plot(profit.svm) # see how profits change as more mailings are made
n.mail.valid <- which.max(profit.svm) # number of mailings that maximizes profits
c(n.mail.valid, max(profit.svm)) # report number of mailings and maximum profit 1314 11596.5

cutoff.svm <- sort(svm.pred, decreasing=T)[n.mail.valid+1] # set cutoff based on n.mail.valid
chat.valid.svm <- ifelse(svm.pred>cutoff.svm, 1, 0) # mail to everyone above the cutoff
table(chat.valid.svm, c.valid) # classification table
# Check 333+981 = 1314
# Check 14.5*981 - 2*1314 = 11596.5
```

```{r}
# We use “maximum profit” as the evaluation criteria to select the final classification model to classify DONR responses in the test dataset.

####### Results #######
# n.mail Profit  Model
# 1345   11621.5 LDA
# 1273   11635   LR
# 1391   11210.5 QDA
# 1305   11208.5 KNN
# 1237   11939   GAM (best)
# 1391   11312   TREE
# 1314   11799.5 RF 
# 1314   11596.5 SVM

# Based on maximum profit, the generalized additive model scores the best as it nets us a profit of $11,939.00 on the test data.
```

### Prediction modeling
Multi-colinearity should be very careful for building regression models, so I removed those variables which are correlated to each other strongly.

#### Model 1: Least squares regression
```{r}
model.ls <- lm(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + wrat + avhv + npro + rgif + tdon + tlag, data.train.std.y)
summary(model.ls) # we can see some predictors are not significant; so I drop them (reg1, reg2, wrat and tlag); also note that the predictor erro and std error will not change much after dropping them
model.ls <- lm(damt ~ reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + avhv + npro + rgif + tdon, data.train.std.y)
summary(model.ls)
pred.valid.ls <- predict(model.ls, data.valid.std.y) # validation predictions
mean((y.valid - pred.valid.ls)^2) # mean prediction error 1.880
sd((y.valid - pred.valid.ls)^2)/sqrt(n.valid.y) # std error 0.169

# Check assumptions
par(mfrow=c(2,2)); plot(model.ls)               
library(car); vif(model.ls) 
# all ok around 1 - multicollinearity not an issue 
```

#### Model 2: Stepwise regression using AIC
```{r}
library(MASS)
model.AIC <- lm(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + wrat + avhv + npro + rgif + tdon + tlag, data.train.std.y)
step <- stepAIC(model.AIC, direction="both")
step$anova # display results; note that the model with smallest AIC is exactly the same as model 1
summary(step) 
model.AIC <- lm(damt ~ reg2 + reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + avhv + npro + rgif + tdon, data.train.std.y)
summary(model.AIC)
pred.valid.aic <- predict(model.AIC, data.valid.std.y) # validation predictions
mean((y.valid - pred.valid.aic)^2) # mean prediction error 1.872
sd((y.valid - pred.valid.aic)^2)/sqrt(n.valid.y) # std error 0.169

# Check assumptions
par(mfrow=c(2,2)); plot(model.AIC)               
library(car); vif(model.AIC)
# all ok around 1 - multicollinearity not an issue 
```

#### Model 3: Best subset selection using BIC
```{r}
library(leaps)
BIC <- regsubsets(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + wrat + avhv + npro + rgif + tdon + tlag, data.train.std.y, nvmax=10) 
summary(BIC)                                                             
# determine the number of variables with the lowest BIC value
which.min(summary(BIC)$bic) 
coef(BIC, id=8)                                   

# Create the function to use predict() with regsubsets 
predict.regsubsets=function(object,newdata,id,...){
   form=as.formula(object$call[[2]])
   mat=model.matrix(form,newdata)
   coefi=coef(object,id=id)
   xvars=names(coefi)
   mat[,xvars]%*%coefi
}

pred.BIC <- predict(BIC, data.valid.std.y, id=8)  # validation predictions       
mean((y.valid-pred.BIC)^2) # mean prediction error 1.895                      
sd((y.valid-pred.BIC)^2)/sqrt(n.valid.y) # std error 0.170    
plot(BIC, scale='bic')
```

#### Model 4: Ridge regression using 10-fold cross-validation
```{r}
library(glmnet) # The package "glmnet" was used to perform Ridge Regression and Lasso
set.seed(2015) 
x <- as.matrix(data.train.std.y[,1:20]) 
y <- as.matrix(data.train.std.y[,21])
valid.x <- as.matrix(data.valid.std.y[,1:20]) 
valid.y <- as.matrix(data.valid.std.y[,21])
cv.out <- cv.glmnet(x, y, alpha=0, nfolds=10)
bestlambda <- cv.out$lambda.1se; bestlambda
ridge <- glmnet(x, y, alpha=0) 
pred.ridge <- predict(ridge, s=bestlambda, valid.x)
mean((valid.y-pred.ridge)^2) # mean prediction error 1.832
sd((valid.y-pred.ridge)^2)/sqrt(n.valid.y) # std error 0.173
plot(cv.out)
```

#### Model 5: Lasso regression using 10-fold cross-validation
```{r}
set.seed(2015)
cv.out <- cv.glmnet(x, y, alpha=1, nfolds=10)
bestlambda <- cv.out$lambda.1se; bestlambda
lasso <- glmnet(x, y, alpha=1)
pred.lasso <- predict(lasso, s=bestlambda, valid.x, exact=T) 
mean((valid.y-pred.lasso)^2) # mean prediction error 1.819
sd((valid.y-pred.lasso)^2)/sqrt(n.valid.y) # std error 0.164 
plot(cv.out)
```

#### Model 6: Principal Components Regression
```{r}
library(pls)
set.seed(2015)
pcr.fit=pcr(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + wrat + avhv + npro + rgif + tdon + tlag, data=data.train.std.y, validation="CV")
summary(pcr.fit)
pred.valid.pcr <- predict(pcr.fit, data.valid.std.y,ncomp=15)
mean((y.valid - pred.valid.pcr)^2) # mean prediction error 1.869997
sd((y.valid - pred.valid.pcr)^2)/sqrt(n.valid.y) # std error 0.1689471
```

#### Model 7: Partial Least Squares
```{r}
set.seed(2015)
pls.fit=plsr(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + I(hinc^2) + genf + wrat + avhv + npro + rgif + tdon + tlag, data=data.train.std.y, validation="CV")
summary(pls.fit)
pred.valid.pls <- predict(pls.fit, data.valid.std.y,ncomp=5)
mean((y.valid - pred.valid.pls)^2) # mean prediction error 1.873125
sd((y.valid - pred.valid.pls)^2)/sqrt(n.valid.y) # std error 0.1693118
```

```{r}
# We will use “mean prediction error” as the evaluation criteria to select the final prediction model to predict DAMT responses in the test dataset.

####### Results #######
# MPE     sd      Model
# 1.880   0.169   LS
# 1.880   0.169   Stepwise
# 1.895   0.170   Best Subset
# 1.832   0.173   Ridge
# 1.819   0.164   Lasso (Best)
# 1.870   0.169   PCR
# 1.873   0.169   PLS

# We can see that the ridge regression and lasso model perform better than the least squares models. This could mean that the least squares estimates have high variance. We also see that the lasso performs better than the ridge regression. 
```
